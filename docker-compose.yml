# ========================================
# IRIS Chat - Docker Compose
# ========================================
# Deploy: docker-compose up --build
# O con podman: podman-compose up --build
# ========================================

services:
  iris-chat:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: iris-chat
    restart: unless-stopped
    # Usar network_mode: host para acceso directo a Ollama
    network_mode: host
    environment:
      - PORT=9999
      - DB_PATH=/data/chat.db
      # Para network_mode: host, usar localhost directamente
      - OLLAMA_URL=http://localhost:11434
      # Modelos disponibles (descomenta el que prefieras):
      # - OLLAMA_MODEL=deepseek-r1:14b    # Requiere ~9GB RAM
      # - OLLAMA_MODEL=deepseek-r1:7b     # Requiere ~5GB RAM
      # - OLLAMA_MODEL=llama3.2:3b        # Requiere ~3GB RAM (recomendado para poca RAM)
      - OLLAMA_MODEL=llama3.2:3b
      - SESSION_DURATION=24h
      - ENABLE_SECURITY_FILTERS=true
      - FORCE_SECURE_COOKIE=false
      - OLLAMA_TIMEOUT=5m
      - OLLAMA_RETRIES=3
    volumes:
      # Volumen persistente para la base de datos SQLite
      - iris-data:/data
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  iris-data:
    driver: local
